"""
Create visual comparison charts between Random Forest and Transformer models.
"""

import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import pandas as pd

# Set style
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

def create_comparison_charts():
    """Create comprehensive comparison visualizations."""
    
    fig = plt.figure(figsize=(20, 15))
    
    # 1. Performance Comparison
    ax1 = plt.subplot(2, 3, 1)
    models = ['Random Forest', 'Transformer', 'SVM RBF', 'Logistic Reg', 'Neural Net']
    binary_acc = [100, 83.33, 83.33, 83.33, 83.33]
    multi_acc = [100, 100, 83.33, 83.33, 83.33]
    
    x = np.arange(len(models))
    width = 0.35
    
    bars1 = ax1.bar(x - width/2, binary_acc, width, label='Binary Accuracy', alpha=0.8)
    bars2 = ax1.bar(x + width/2, multi_acc, width, label='Multi-class Accuracy', alpha=0.8)
    
    ax1.set_title('Model Accuracy Comparison', fontsize=14, fontweight='bold')
    ax1.set_ylabel('Accuracy (%)')
    ax1.set_xlabel('Models')
    ax1.set_xticks(x)
    ax1.set_xticklabels(models, rotation=45)
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # Add value labels on bars
    for bar in bars1:
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height + 1,
                f'{height:.1f}%', ha='center', va='bottom', fontsize=9)
    
    for bar in bars2:
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height + 1,
                f'{height:.1f}%', ha='center', va='bottom', fontsize=9)
    
    # 2. Training Time Comparison
    ax2 = plt.subplot(2, 3, 2)
    training_times = [0.1, 0.95, 0.05, 0.03, 0.2]
    colors = ['green', 'red', 'blue', 'orange', 'purple']
    
    bars = ax2.bar(models, training_times, color=colors, alpha=0.7)
    ax2.set_title('Training Time Comparison', fontsize=14, fontweight='bold')
    ax2.set_ylabel('Training Time (seconds)')
    ax2.set_xlabel('Models')
    ax2.set_xticks(range(len(models)))
    ax2.set_xticklabels(models, rotation=45)
    ax2.grid(True, alpha=0.3)
    
    # Add value labels
    for bar in bars:
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.02,
                f'{height:.2f}s', ha='center', va='bottom', fontsize=9)
    
    # 3. Model Complexity (Parameters)
    ax3 = plt.subplot(2, 3, 3)
    parameters = [1000, 548933, 100, 20, 5000]
    
    # Use log scale for better visualization
    bars = ax3.bar(models, parameters, alpha=0.7)
    ax3.set_title('Model Complexity (Parameters)', fontsize=14, fontweight='bold')
    ax3.set_ylabel('Number of Parameters (log scale)')
    ax3.set_xlabel('Models')
    ax3.set_yscale('log')
    ax3.set_xticks(range(len(models)))
    ax3.set_xticklabels(models, rotation=45)
    ax3.grid(True, alpha=0.3)
    
    # Add value labels
    for i, (bar, param) in enumerate(zip(bars, parameters)):
        height = bar.get_height()
        if param >= 1000:
            label = f'{param//1000}K' if param < 1000000 else f'{param//1000000}M'
        else:
            label = str(param)
        ax3.text(bar.get_x() + bar.get_width()/2., height * 1.1,
                label, ha='center', va='bottom', fontsize=9)
    
    # 4. Efficiency Radar Chart
    ax4 = plt.subplot(2, 3, 4, projection='polar')
    
    # Metrics: Speed, Memory, Accuracy, Interpretability, Deployment Ease
    metrics = ['Speed', 'Memory\nEfficiency', 'Accuracy', 'Interpretability', 'Deployment\nEase']
    
    # RF scores (out of 5)
    rf_scores = [5, 5, 5, 5, 5]
    # Transformer scores
    tf_scores = [3, 2, 4, 3, 2]
    
    angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()
    angles += angles[:1]  # Complete the circle
    
    rf_scores += rf_scores[:1]
    tf_scores += tf_scores[:1]
    
    ax4.plot(angles, rf_scores, 'o-', linewidth=2, label='Random Forest', color='green')
    ax4.fill(angles, rf_scores, alpha=0.25, color='green')
    ax4.plot(angles, tf_scores, 'o-', linewidth=2, label='Transformer', color='red')
    ax4.fill(angles, tf_scores, alpha=0.25, color='red')
    
    ax4.set_xticks(angles[:-1])
    ax4.set_xticklabels(metrics)
    ax4.set_ylim(0, 5)
    ax4.set_title('Overall Efficiency Comparison', fontsize=14, fontweight='bold', y=1.08)
    ax4.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))
    ax4.grid(True)
    
    # 5. Resource Usage Comparison
    ax5 = plt.subplot(2, 3, 5)
    
    # Normalized resource usage (RF = 1, others relative to RF)
    categories = ['Training\nTime', 'Memory\nUsage', 'Parameters', 'Inference\nSpeed', 'Energy\nUsage']
    rf_usage = [1, 1, 1, 1, 1]  # Baseline
    tf_usage = [10, 42, 549, 1000, 100]  # Relative to RF
    
    x = np.arange(len(categories))
    width = 0.35
    
    bars1 = ax5.bar(x - width/2, rf_usage, width, label='Random Forest', alpha=0.8, color='green')
    bars2 = ax5.bar(x + width/2, tf_usage, width, label='Transformer', alpha=0.8, color='red')
    
    ax5.set_title('Resource Usage (Relative to Random Forest)', fontsize=14, fontweight='bold')
    ax5.set_ylabel('Relative Usage (log scale)')
    ax5.set_xlabel('Resource Type')
    ax5.set_yscale('log')
    ax5.set_xticks(x)
    ax5.set_xticklabels(categories)
    ax5.legend()
    ax5.grid(True, alpha=0.3)
    
    # Add value labels
    for bar in bars1:
        height = bar.get_height()
        ax5.text(bar.get_x() + bar.get_width()/2., height * 1.1,
                f'{height}x', ha='center', va='bottom', fontsize=9)
    
    for bar, usage in zip(bars2, tf_usage):
        height = bar.get_height()
        ax5.text(bar.get_x() + bar.get_width()/2., height * 1.1,
                f'{usage}x', ha='center', va='bottom', fontsize=9)
    
    # 6. Use Case Recommendations
    ax6 = plt.subplot(2, 3, 6)
    ax6.axis('off')
    
    recommendation_text = """
    ðŸ“Š RECOMMENDATION SUMMARY
    
    ðŸ† RANDOM FOREST: OPTIMAL CHOICE
    
    âœ… Current Project Winner:
    â€¢ Perfect 100% accuracy
    â€¢ 10x faster training
    â€¢ 42x smaller memory footprint
    â€¢ Production-ready immediately
    â€¢ Interpretable decisions
    
    ðŸ¤– TRANSFORMER: FUTURE POTENTIAL
    
    âš¡ Consider when:
    â€¢ Dataset size > 1,000 samples
    â€¢ Complex pattern recognition needed
    â€¢ Advanced analytics required
    â€¢ Research & development focus
    
    ðŸŽ¯ CONCLUSION:
    Random Forest provides the optimal
    balance of performance, efficiency,
    and practicality for current needs.
    """
    
    ax6.text(0.05, 0.95, recommendation_text, transform=ax6.transAxes,
             fontsize=11, verticalalignment='top', fontfamily='monospace',
             bbox=dict(boxstyle="round,pad=0.5", facecolor="lightblue", alpha=0.8))
    
    plt.tight_layout()
    
    # Save the plot
    plt.savefig('results/transformer/model_comparison_comprehensive.png', 
                dpi=300, bbox_inches='tight', facecolor='white')
    plt.show()
    
    print("ðŸ“Š Comprehensive comparison charts created and saved!")

def create_summary_table():
    """Create a detailed comparison table."""
    
    comparison_data = {
        'Metric': [
            'Binary Accuracy', 'Multi-class Accuracy', 'Training Time', 'Parameters',
            'Memory Usage', 'Inference Speed', 'Interpretability', 'Deployment Ease',
            'Energy Efficiency', 'Maintenance Cost'
        ],
        'Random Forest': [
            '100%', '100%', '< 0.1s', '~1,000', '~50 KB', '~0.001ms',
            'Excellent', 'Very Easy', 'Very High', 'Low'
        ],
        'Transformer': [
            '83.33%', '100%', '0.95s', '548,933', '~2.1 MB', '~1.0ms',
            'Good', 'Complex', 'Moderate', 'High'
        ],
        'Winner': [
            'ðŸ† RF', 'ðŸ¤ Tie', 'ðŸ† RF', 'ðŸ† RF', 'ðŸ† RF', 'ðŸ† RF',
            'ðŸ† RF', 'ðŸ† RF', 'ðŸ† RF', 'ðŸ† RF'
        ]
    }
    
    df = pd.DataFrame(comparison_data)
    
    # Create table visualization
    fig, ax = plt.subplots(figsize=(12, 8))
    ax.axis('tight')
    ax.axis('off')
    
    table = ax.table(cellText=df.values, colLabels=df.columns,
                     cellLoc='center', loc='center')
    table.auto_set_font_size(False)
    table.set_fontsize(11)
    table.scale(1.2, 2)
    
    # Style the table
    for i in range(len(df.columns)):
        table[(0, i)].set_facecolor('#4CAF50')
        table[(0, i)].set_text_props(weight='bold', color='white')
    
    # Color code the winner column
    for i in range(1, len(df) + 1):
        if 'ðŸ† RF' in df.iloc[i-1]['Winner']:
            table[(i, 3)].set_facecolor('#C8E6C9')
        elif 'ðŸ¤ Tie' in df.iloc[i-1]['Winner']:
            table[(i, 3)].set_facecolor('#FFF9C4')
    
    plt.title('Detailed Model Comparison Table', fontsize=16, fontweight='bold', pad=20)
    plt.savefig('results/transformer/detailed_comparison_table.png', 
                dpi=300, bbox_inches='tight', facecolor='white')
    plt.show()
    
    print("ðŸ“‹ Detailed comparison table created!")

if __name__ == "__main__":
    import os
    os.makedirs('results/transformer', exist_ok=True)
    
    create_comparison_charts()
    create_summary_table()
    
    print("\nðŸŽ‰ All comparison visualizations completed!")
    print("ðŸ“ Files saved to: results/transformer/") 